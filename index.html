<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Deep Learning Restoration</title>
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body>
        <div class="container mt-5">
            <div class="row">
                <div class="col-lg-8">
                    <article>
                        <header class="mb-4">
                            <h1 class="fw-bolder mb-1">Deep Learning Restoration</h1>

                        </header>
                        <figure class="mb-4"><img class="img-fluid rounded" src="images/download.jfif" alt="..." /></figure>
                        <section class="mb-5">
                            <h3>IBM Definition of Neural Networks</h3>
                            <p class="fs-5 mb-4">Neural networks, also known as artificial neural networks (ANNs) or simulated neural networks (SNNs), are a subset of machine learning and are at the heart of deep learning algorithms. Their name and structure are inspired by the human brain, mimicking the way that biological neurons signal to one another.<br><br>
                            Artificial neural networks (ANNs) are comprised of a node layers, containing an input layer, one or more hidden layers, and an output layer. Each node, or artificial neuron, connects to another and has an associated weight and threshold. If the output of any individual node is above the specified threshold value, that node is activated, sending data to the next layer of the network. Otherwise, no data is passed along to the next layer of the network.<br> 
                            Source: <a href="https://www.ibm.com/cloud/learn/neural-networks">IBM Definition Neural Networks</a></p><br>
                            
                            <h3>Nvidia Neural Network Image Restoration Research Paper</h3>
                            <p class="fs-5 mb-4">Neural networks are becoming central in several areas of computer vision and image processing and different architectures have been proposed to solve specific problems. The impact of the loss layer of neural networks, however, has not received much attention in the context of image processing: the default and virtually only choice is 2. In this paper, we bring attention to alternative choices for image restoration. In particular, we show the importance of perceptually-motivated losses when the resulting image is to be evaluated by a human observer. We compare the performance of several losses, and propose a novel, differentiable error function. We show that the quality of the results improves significantly with better loss functions, even when the network architecture is left unchanged.<br>
                            Source: <a href="https://on-demand.gputechconf.com/gtc/2017/presentation/s7447-orazio-gallo-image-restoration-with-neural-networks.pdf">Nvidia Research Paper</a></p><br>
                            
                            <h3>GPU Pricing Chart</h3>
                            <p class="fs-5 mb-4">Across the data we’ve been collecting for months now, January has been one of the better months for Nvidia GPUs. Prices were flat on average in December, increased 6% month on month in November, and declined slightly in October. What we’re seeing in January is the largest decline in current-gen Nvidia GPU prices since July, and for some products like the RTX 3070 and RTX 3060, street prices are close to their lowest point in the period we’ve been capturing data. Inflation remains high, most Nvidia GPUs are still double their MSRP, but we think it’s important to mention the positives as well, like how GPU prices have fallen significantly since they peaked in May.<br><br>
                            Also, since we’ve been collecting this data, January had one of the highest rates of listings going unsold for Nvidia GPUs, suggesting that some of the high prices seen on the scalper market are being less tolerated now than in previous months, which should lead to further price reductions in the future if that trend continues.<br>
                            Source: <a href="https://www.techspot.com/article/2397-gpu-pricing-2022-update/">GPU Pricing Chart</a></p><br>
                            
                            <h3>SCRNN Image Neural Network Upscaler</h3>
                            <p class="fs-5 mb-4"> The SRCNN is a deep convolutional neural network that learns the end-to-end mapping of low-resolution to high-resolution images. As a result, we can use it to improve the image quality of low-resolution images. The goal of super-resolution (SR) is to recover a high-resolution image from a low-resolution input. The authors of the SRCNN describe their network, pointing out the equivalence of their method to the sparse-coding method4, which is a widely used learning method for image SR. This is an important and educational aspect of their work, because it shows how example-based learning methods can be adapted and generalized to CNN models.<br>
                            Source: <a href="https://www.techspot.com/article/2397-gpu-pricing-2022-update/">SCRNN Image Upscaler</a></p><br>
                            
                            <h3>Training a Neural Network With MNIST</h3>
                            <p class="fs-5 mb-4"> The MNIST database, an extension of the NIST database, is a low-complexity data collection of handwritten digits used to train and test various supervised machine learning algorithms. The database contains 70,000 28x28 black and white images representing the digits zero through nine. The data is split into two subsets, with 60,000 images belonging to the training set and 10,000 images belonging to the testing set. The separation of images ensures that given what an adequately trained model has learned previously, it can accurately classify relevant images not previously examined. 
                                <br><br> 
                            In simple terms, MNIST can be thought of as the “Hello, World!” of machine learning. MNIST is primarily used to experiment with different machine learning algorithms and to compare their relative strengths. Yann LeCun, one of the three researchers behind the creation of MNIST, has devoted a portion of his research to using MNIST to experiment with cutting edge algorithms, which can be seen on his personal website yann.lecun.com. Many researchers, hobbyists, and students alike continue to use MNIST alongside their algorithmic implementations and other popular datasets as a way to solidify their understanding of the fundamental concepts in machine learning and to compare their new algorithms against existing cutting edge research.<br>
                            Source: <a href="https://deepai.org/dataset/mnist">MNIST Training Set</a></p><br>
                            
                            <h3>TensorFlow Open Source Neural Network</h3>
                            <p class="fs-5 mb-4"> TensorFlow makes it easy for beginners and experts to create machine learning models for desktop, mobile, web, and cloud. A neural network is a type of model that can be trained to recognize patterns. It is composed of layers, including input and output layers, and at least one hidden layer. Neurons in each layer learn increasingly abstract representations of the data. For example, in this visual diagram we see neurons detecting lines, shapes, and textures. These representations (or learned features) make it possible to classify the data. 
                                <br><br> 
                            Neural networks are trained by gradient descent. The weights in each layer begin with random values, and these are iteratively improved over time to make the network more accurate. A loss function is used to quantify how inaccurate the network is, and a procedure called backpropagation is used to determine whether each weight should be increased, or decreased, to reduce the loss.<br>
                            Source: <a href="https://www.tensorflow.org/learn">TensorFlow Learning Neural Networks</a></p><br>
                            
                            

                        </section>
                    </article>                    
                </div>
                <div class="col-lg-4">
                    <div class="card mb-4">
                        <div class="card-header">Quick Links</div>
                        <div class="card-body">
                            <div class="row">
                                <div class="col-sm-10">
                                    <ul class="list-unstyled">
                                        <li><a href="https://www.ibm.com/cloud/learn/neural-networks">IBM Definition Neural Networks</a></li>
                                        <li><a href="https://on-demand.gputechconf.com/gtc/2017/presentation/s7447-orazio-gallo-image-restoration-with-neural-networks.pdf">Nvidia Research Paper</a></li>
                                        <li><a href="https://www.techspot.com/article/2397-gpu-pricing-2022-update/">GPU Pricing Chart</a></li>
                                        <li><a href="https://www.techspot.com/article/2397-gpu-pricing-2022-update/">SCRNN Image Upscaler</a></li>
                                        <li><a href="https://deepai.org/dataset/mnist">MNIST Training Set</a></li>
                                        <li><a href="https://www.tensorflow.org/learn">TensorFlow Learning Neural Networks</a></li>
                                    </ul>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js"></script>

    </body>
</html>
